{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af473fba-4477-4702-bb24-9042b5335882",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Implement Reinforcement Learning using an example of a maze environment that the agent needs to explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ca79df-12d5-4724-be00-391691bbb3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the maze environment\n",
    "class Maze:\n",
    "    def __init__(self, size, start, goal, walls):\n",
    "        self.size = size\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.walls = walls\n",
    "        self.state = start\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.state == self.goal\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        actions = ['up', 'down', 'left', 'right']\n",
    "        x, y = self.state\n",
    "        if x == 0:\n",
    "            actions.remove('up')\n",
    "        if x == self.size[0] - 1:\n",
    "            actions.remove('down')\n",
    "        if y == 0:\n",
    "            actions.remove('left')\n",
    "        if y == self.size[1] - 1:\n",
    "            actions.remove('right')\n",
    "        return actions\n",
    "\n",
    "    def take_action(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 'up':\n",
    "            new_state = (x - 1, y)\n",
    "        elif action == 'down':\n",
    "            new_state = (x + 1, y)\n",
    "        elif action == 'left':\n",
    "            new_state = (x, y - 1)\n",
    "        elif action == 'right':\n",
    "            new_state = (x, y + 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        \n",
    "        if new_state in self.walls or not (0 <= new_state[0] < self.size[0] and 0 <= new_state[1] < self.size[1]):\n",
    "            new_state = self.state\n",
    "        \n",
    "        self.state = new_state\n",
    "        reward = 1 if self.state == self.goal else -0.01\n",
    "        return new_state, reward\n",
    "\n",
    "    def render(self):\n",
    "        maze = np.zeros(self.size)\n",
    "        maze[self.goal] = 2\n",
    "        for wall in self.walls:\n",
    "            maze[wall] = -1\n",
    "        maze[self.state] = 1\n",
    "        plt.imshow(maze, cmap='cool', vmin=-1, vmax=2)\n",
    "        plt.show()\n",
    "\n",
    "# Q-learning algorithm\n",
    "class QLearning:\n",
    "    def __init__(self, maze, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.maze = maze\n",
    "        # Fixed number of actions ('up', 'down', 'left', 'right')\n",
    "        self.q_table = np.zeros((*maze.size, 4))  # Fixed to 4 actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = ['up', 'down', 'left', 'right']  # Fixed actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        possible_actions = self.maze.get_possible_actions()  # Get possible actions for the current state\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(possible_actions)  # Choose a valid random action\n",
    "        else:\n",
    "            # Only select the action with the highest Q-value from possible actions\n",
    "            action_values = {action: self.q_table[state[0], state[1], self.actions.index(action)] for action in possible_actions}\n",
    "            return max(action_values, key=action_values.get)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        action_idx = self.actions.index(action)\n",
    "        best_next_action = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        self.q_table[state[0], state[1], action_idx] += self.alpha * (reward + self.gamma * best_next_action - self.q_table[state[0], state[1], action_idx])\n",
    "\n",
    "    def train(self, episodes):\n",
    "        for _ in range(episodes):\n",
    "            state = self.maze.reset()\n",
    "            while not self.maze.is_terminal():\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward = self.maze.take_action(action)\n",
    "                self.update_q_table(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    size = (5, 5)\n",
    "    start = (0, 0)\n",
    "    goal = (4, 4)\n",
    "    walls = [(2, 2), (3, 2), (1, 3)]\n",
    "    \n",
    "    maze = Maze(size, start, goal, walls)\n",
    "    q_learning = QLearning(maze, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "    q_learning.train(1000)\n",
    "    \n",
    "    # Visualization of the learned policy\n",
    "    policy = np.full(size, '', dtype=object)\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            state = (i, j)\n",
    "            if state == goal:\n",
    "                policy[i, j] = 'G'\n",
    "            elif state in walls:\n",
    "                policy[i, j] = 'W'\n",
    "            else:\n",
    "                action_idx = np.argmax(q_learning.q_table[i, j])\n",
    "                policy[i, j] = q_learning.actions[action_idx][0].upper()\n",
    "    \n",
    "    plt.imshow(np.where(policy == 'W', -1, np.where(policy == 'G', 2, 0)), cmap='cool', vmin=-1, vmax=2)\n",
    "    plt.title('Learned Policy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cfdda-f1a9-4942-a696-4904435a251c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
